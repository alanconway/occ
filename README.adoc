= Cluster For Clouds Using Kubernetes™
:toc: preamble

The *ClusterFCUK*™ (formerly know as the Old Crap Cluster™) is a
bare-metal kubernetes cluster consisting of discarded laptops running
Fedora 29. This describes how I set it up, for anyone who cares about
such things.

.The ClusterFCUK
image::old_crap_cluster.jpg[ClusterFCUK, ClusterFCUK, caption="",  ClusterFcuk", width=75%]

== Setting up the cluster

Plargarised from the kubeadm
guide.footnote:[From https://kubernetes.io/docs/setup/independent/install-kubeadm,
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm]
and other sources.

Most steps are done over `ssh` from a separate "console"
workstation. All hosts (console and servers) must be able to resolve
each other’s names on your network. I used DHCP reservations on my
home router and a static `/etc/hosts` file. If you have a less crappy
router you may have better options.

Configuration files:

- `./config/` kubernetes config files.
- `./etc/`  files to be copied to /etc on cluster servers.

You should review all the config files, you *must* modify these two to
match your network:

- `./etc/hosts`
- `./config/metallb.yml`

=== Prepare the console

Assuming your servers are called `master`, `node1`, and `node2`,
you should set the console environment as follows:
....
 HOSTS="node1 node2 master"
....

Install `ssh` and the kubernetes client
....
sudo dnf install -y openssh-clients kubernetes-client
....

Set up an ssh key-pair for certificate-based login, for example using
http://fedoranews.org/dowen/sshkeys/[this guide].

Prepare the servers
~~~~~~~~~~~~~~~~~~~

Log into each each to server enable ssh and disable suspend (for laptops).
Note this should be the last time you need to touch the servers.

....
# On each server
sudo hostnamectl set-hostname # set an appropriate name
systemctl enable --now sshd
systemctl mask --now sleep.target suspend.target hibernate.target hybrid-sleep.target
....

NOTE: The remaining steps are done from the console.

Enable no-password ssh login for self and root.

- You need an SSH key pair in `~/.ssh` on the console workstation.
- You’ll have to enter passwords this one time, but no more.

....
for h in $HOSTS; do
    ssh-copy-id $h
    ssh $h sudo -S cp -r $HOME/.ssh /root
done
....

Disable swapping (required by `kubeadm`) and firewalls:

....
for h in $HOSTS; do
    ssh root@$h swapoff -a # kubeadm wants no swapping
    ssh root@$h "sed -i -E 's/(^.*\\<swap\\>.*$)/# \\1/' /etc/fstab"
    ssh root@$h systemctl mask firewalld
done
....

Upgrade packages

....
for h in $HOSTS; do
    ssh -n root@$h dnf -y update&
done
wait; echo done
....

NOTE: In theory you can keep firwalls running with a
https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports[limited set of ports].

=== Install packages

NOTE: Fedora `docker` package is https://kubernetes.io/docs/setup/cri[too old] for k8s. +
On Fedora 29 use `docker-ce`, on Fedora 30 use `moby-engine`.

Remove old docker if already installed:

....
for h in $HOSTS; do
    ssh -n root@$h 'dnf erase -y docker docker-* && rm -rf /etc/docker'
done
wait; echo done
....

For Fedora 29:
....
for h in $HOSTS; do
    ssh -n root@$h 'dnf -y install dnf-plugins-core && \
    dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo && \
    dnf install -y docker-ce docker-ce-cli'
done
wait; echo done
....

For Fedora 30:
....
for h in $HOSTS; do
  ssh -n root@$h dnf -y install moby-engine &
done
wait; echo done
....

Install other packages.

....
for h in $HOSTS; do
    ssh -n root@$h dnf -y install containerd ipvsadm kubernetes-kubeadm&
done
wait; echo done
for h in $HOSTS; do ssh root@$h 'systemctl enable --now docker kubelet && systemctl is-active docker'; done
....

NOTE: kubelet will report errors in the system log file until the master is initialized in the next section.

Cluster configuration
~~~~~~~~~~~~~~~~~~~~~

Copy configuration files from ./etc

....
for h in $HOSTS; do scp -r etc root@$h:/; done
....

Initialize master, copy administrator config and install a network overlay. I used `flannel`, there are
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network[other options].

....
# --pod-network-cidr=10.244.0.0/16 is for flannel.
# --ignore-preflight is to ignore _unsupported kernel version_ warnings
ssh -lroot master kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=SystemVerification | tee kubeadm-init
# Copy admin.conf to the console so kubectl will talk to our cluster.
scp root@master:/etc/kubernetes/admin.conf ~/.kube/config
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
bin/wait-pods --all-namespaces
....

Join nodes to the master using the `kubeadm-init` file that was generated by `kubeadm init`:

....
JOIN_CMD=$(grep 'kubeadm join .* --token' kubeadm-init)
for h in node1 node2; do
    ssh root@$h $JOIN_CMD --ignore-preflight-errors=SystemVerification
    ssh root@$h mkdir -p /etc/kubernetes/manifests
done
bin/wait-pods --all-namespaces
kubectl get nodes
....

Install a load balancer. I picked metallb - https://metallb.universe.tf/

NOTE: you must upate `config/metallb.yml` for your network.

....
kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml
kubectl apply -f config/metallb.yml
bin/wait-pods --all-namespaces
....

Test the cluster
~~~~~~~~~~~~~~~~

Deploy hello-world:

....
kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
....

Expose hello-world as a NodePort - does not require a load balancer.

....
kubectl expose deployment hello-node --type=NodePort --port=8080
PORT=$(kubectl get svc hello-node -o=jsonpath='{.spec.ports[?(@.port==8080)].nodePort}')
curl master:$PORT
kubectl delete svc hello-node
....

Expose hello-world as a LoadBalancer - needs a load balancer configured.

....
kubectl expose deployment hello-node --type=LoadBalancer --port=8080
kubectl get svc hello-node # Wait till it has an external IP
IP_ADDR=$(kubectl get svc hello-node -o=go-template='{{index .status.loadBalancer.ingress 0 "ip"}}')
curl $IP_ADDR:8080
kubectl delete deployment hello-node
....

Run Sonobuoy compliance check from https://github.com/heptio/sonobuoy

....
go get github.com/heptio/sonobuoy

# Quick test set
sonobuoy run --wait --mode quick
sonobuoy e2e $(sonobuoy retrieve)

# Full test set
sonobuoy run --wait
sonobuoy e2e $(sonobuoy retrieve)

# Cleanup
sonobuoy delete --wait --all
....

NOTE: No idea what this checks, but it passed so we are _compliant_!

Use a private docker registry
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To use your own `dockerhub.io` account as a registry for your cluster.footnote:[From https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry]

....
docker login # Adds credentails to ~/.docker/config.json
for h in $HOSTS; do
    scp ~/.docker/config.json root@$h:/var/lib/kubelet/config.json
done
....

Now you can pull images using your dockerhub username as the repository.

Adding useful features
----------------------

First create an admin service account for use in other installs.

....
kubectl apply -f config/admin.yml
....

Kubernetes dashboard
~~~~~~~~~~~~~~~~~~~~

Apply dashboard as admin, copy token so you can log in via `~/.kube/config`

....
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
TOKEN=$(kubectl -n kube-system describe secret admin | awk '$1=="token:"{print $2}')
kubectl config set-credentials kubernetes-admin --token="${TOKEN}"
kubectl proxy &
xdg-open 'http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login'
....

Install Knative serving and eventing from repo
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

....
docker login
export KO_DOCKER_REPO=$(docker info | awk '/Username: / {print $2}')
PATH=$PATH:$PWD/bin

cd $HOME/go/src/github.com/knative
# Set a branch/tag
# REV=master for d in *; do git -C $d checkout $REV; done

# ISTIO
kubectl apply -f serving/third_party/istio-1.1-latest/istio-crds.yaml
wait-crd gateways.networking.istio.io
kubectl apply -f serving/third_party/istio-1.1-latest/istio.yaml
wait-pods --all-namespaces

# Cert manager
kubectl apply -f serving/third_party/cert-manager-0.6.1/cert-manager-crds.yaml
wait-crd certificates.certmanager.k8s.io
kubectl apply -f serving/third_party/cert-manager-0.6.1/cert-manager.yaml --validate=false
wait-pods --all-namespaces

# Serving
ko apply -f serving/config/ && wait-pods --all-namespaces
ko apply -f serving/config/monitoring && wait-pods --all-namespaces

# Logging and monitoring
kubectl apply -R -f serving/config/monitoring/100-namespace.yaml && wait-pods --all-namespaces
kubectl apply -R -f serving/third_party/config/monitoring/logging/elasticsearch && wait-pods --all-namespaces
kubectl apply -R -f serving/config/monitoring/logging/elasticsearch && wait-pods --all-namespaces
kubectl apply -R -f serving/third_party/config/monitoring/metrics/prometheus && wait-pods --all-namespaces
kubectl apply -R -f serving/config/monitoring/metrics/prometheus && wait-pods --all-namespaces
kubectl apply -R -f serving/config/monitoring/tracing/zipkin && wait-pods --all-namespaces

# Eventing
ko apply -f eventing/config/ && wait-pods --all-namespaces
ko apply -f eventing/config/provisioners/in-memory-channel/ && wait-pods --all-namespaces
ko apply -f eventing-sources/config/ && wait-pods --all-namespaces

# Run eventing end-to-end tests
cd eventing
sh test/upload-test-images.sh latest
time go test -v -tags=e2e -count=1 -short -parallel=1 ./...
....

Re-setting the cluster
----------------------

This should shut down your cluster and remove all config changes made by
kubeadm:

....
for h in node1 node2&& do
    kubectl drain $h --delete-local-data --force --ignore-daemonsets
    kubectl delete node $h
done
for h in $HOSTS; do ssh root@$h kubeadm reset --force; done
....

The following extra steps are for people with trust issues. I am not
paranoid, they really are out to get me. Don’t come crying to me if this
turns your servers into paperweights.

....
for h in $HOSTS; do
    ssh root@$h "dnf erase -y 'kube*' flannel etcd docker-ce docker-ce-cli"
    ssh root@$h rm -rfv /etc/kubernetes /usr/libexec/kubernetes /etc/docker */var/lib/docker /usr/libexec/docker /etc/etcd /etc/sysconfig/flanneld /var/lib/cni
    ssh root@$h 'iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X'
done
for h in $HOSTS; do ssh root@$h reboot now; done
....

Other resources
---------------

The following guides are similar to this one, that I found them later: *
https://unofficial-kubernetes.readthedocs.io/en/latest/getting-started-guides/kubeadm/
*
https://developer.ibm.com/tutorials/developing-a-kubernetes-application-with-local-and-remote-clusters/
