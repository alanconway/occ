# Create fedora bare-metal cluster with kubeadm

Short-cut instructions that work for me, your mileage may vary.
Based on
- https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm
- https://kubernetes.io/docs/setup/independent/install-kubeadm

Setup is done via `ssh` from a "console" workstation, which should not
be part of the cluster. You need an SSH certificate on the console.

Console and servers need to be able to resolve each other's names.  I
used DHCP reservations on my router to get stable IP addresses and a
static /etc/hosts file for lookup.

Configuration files:
* config/ kubernetes config files
* etc/ config files for /etc on cluster servers
* save/ local copy of the original web guide for safe keeping.

You should review all the config files, you **must** modify:

* `./etc/hosts`
* `./config/metallb.yml`

From here on I assume that servers with names (or aliases) `master`,
`node1`, and `node2`. The environment variable HOSTS shoul be set to:

     HOSTS="node1 node2 master"

## Prepare the console

The console needs
- `ssh`
- a valid `.ssh` key-pair for no-password login.
- `sudo dnf install -y kubernetes-client`

## Prepare the servers

Do a normal Fedora workstation install on each server, make sure to set the `hostname`.

On each server enable ssh and disable suspend if using laptops.

```
systemctl enable --now sshd
systemctl mask --now sleep.target suspend.target hibernate.target hybrid-sleep.target
```

The remaining steps are done from the console workstation via `ssh`.

Enable no-password ssh login for self and root. You'll have to enter
passwords only for this step. You need an SSH key pair in ~/.ssh on
the console workstation.

```
for h in $HOSTS; do
    ssh-copy-id $h
    ssh $h sudo -S cp -r $HOME/.ssh /root
done
```

Disable swapping (required by kubeadm)

```
for h in $HOSTS; do
    ssh root@$h swapoff -a # kubeadm wants no swapping
    ssh root@$h "sed -i -E 's/(^.*\\<swap\\>.*$)/# \\1/' /etc/fstab"
done
```

Turn off firewalls

```
for h in $HOSTS; do sudo systemctl mask firewalld; done
```

NOTE: It should be possible to open a limited set of ports but I haven't got that working yet. See: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports


## Install packages

NOTE: from  https://kubernetes.io/docs/setup/cri
- Fedora 29 packaged docker (1.13.1) is to old for kubernetes, use fedora-ce
- We will configure docker for systemd in the next section
- On fedora 30 you should `dnf install moby-engine` instead of docker-ce.

Remove old docker, add docker-ce repository:

```
for h in $HOSTS; do
    ssh root@$h 'dnf erase -y docker docker-* && rm -rf /etc/docker' && dnf -y install dnf-plugins-core && dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo' &
done
wait; echo done
```

Install packages.

```
for h in $HOSTS; do
    ssh -n root@$h dnf install -y docker-ce docker-ce-cli containerd.io ipvsadm kubernetes-kubeadm &
done
wait; echo done
for h in $HOSTS; do ssh root@$h 'systemctl enable --now docker kubelet && systemctl is-active docker; done
```

NOTE: kubelet will not start correctly till the kubeadm init step in next section


## Cluster configuration

Copy configuration files from ./etc
```
for h in $HOSTS; do scp -r etc root@$h:/; done
```

Initialize master, copy administrator config file to console so you can run kubectl.

NOTE: Using flannel network overlay
* Other options at https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network
* For flannel to work correctly, you must pass --pod-network-cidr=10.244.0.0/16 to kubeadm init.
* --ignore-preflight ignore for "unsupported kernel version" warnings

```
ssh -lroot master kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=SystemVerification | tee kubeadm-init
scp root@master:/etc/kubernetes/admin.conf ~/.kube/config
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
bin/wait-pods --all-namespaces
```

Join nodes to the master.

NOTE: local `kubeadm-init file was generated by master kubeadm init step above.

```
JOIN_CMD=$(grep 'kubeadm join .* --token' kubeadm-init)
for h in node1 node2; do
    ssh root@$h $JOIN_CMD --ignore-preflight-errors=SystemVerification
    ssh root@$h mkdir -p /etc/kubernetes/manifests
done
bin/wait-pods --all-namespaces
kubectl get nodes
```

Install a load balancer. I picked metallb - https://metallb.universe.tf/

NOTE: you must upate config/metallb.yml for your network.

```
kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml
kubectl apply -f config/metallb.yml
bin/wait-pods --all-namespaces
```


## Test the cluster

Deploy hello-world:
```
kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
```

Expose hello-world as a NodePort - does not require a load balancer.
```
kubectl expose deployment hello-node --type=NodePort --port=8080
PORT=$(kubectl get svc hello-node -o=jsonpath='{.spec.ports[?(@.port==8080)].nodePort}')
curl master:$PORT
kubectl delete svc hello-node
```

Expose hello-world as a LoadBalancer - needs a load balancer configured.
```
kubectl expose deployment hello-node --type=LoadBalancer --port=8080
kubectl get svc hello-node # Wait till it has an external IP
IP_ADDR=$(kubectl get svc hello-node -o=go-template='{{index .status.loadBalancer.ingress 0 "ip"}}')
curl $IP_ADDR:8080
kubectl delete deployment hello-node
```

Run Sonobuoy compliance check from https://github.com/heptio/sonobuoy

```
go get github.com/heptio/sonobuoy

# Quick test set
sonobuoy run --wait --mode quick
sonobuoy e2e $(sonobuoy retrieve)

# Full test set
sonobuoy run --wait
sonobuoy e2e $(sonobuoy retrieve)

# Cleanup
sonobuoy delete --wait --all
```

NOTE: No idea what this checks, but it passed so we are Compliant with something!

## Use a private docker registry

With a multi-node cluster you need to pull images from an external registry,
the minikube/minishift short-cut of using the cluster's docker daemon does not work.

Here's how to use your own dockerhub.io account as a registry:
https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry

```
docker login # Adds credentails to ~/.docker/config.json
for h in $HOSTS; do scp ~/.docker/config.json root@$h:/var/lib/kubelet/config.json; done
# Now you can pull images using your dockerhub username as the repository.
```

# Adding useful features

First create an admin service account for use in other installs.

```
kubectl apply -f config/admin.yml
```

## Kubernetes dashboard

Apply dashboard as admin, copy token so you can log in via ~/.kube/config

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
TOKEN=$(kubectl -n kube-system describe secret admin | awk '$1=="token:"{print $2}')
kubectl config set-credentials kubernetes-admin --token="${TOKEN}"
kubectl proxy &
xdg-open 'http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login'
```

## Helm and prometheus

Taken from
- https://helm.sh/docs/using_helm/#installing-helm
o- https://helm.sh/docs/using_helm/#tiller-and-role-based-access-control

```
curl -L https://git.io/gp0het_helm.sh | bash
helm init --wait --service-account admin --history-max 200 --upgrade

helm install --namespace prometheus stable/prometheus
```

FIXME: stuck in pending state. Need to clean up URL access.

## Install Knative from repo

```
docker login
export KO_DOCKER_REPO=$(docker info | awk '/Username: / {print $2}')
PATH=$PATH:$PWD/bin

cd $HOME/go/src/github.com/knative

kubectl apply -f serving/third_party/istio-1.1-latest/istio-crds.yaml
wait-crd gateways.networking.istio.io
kubectl apply -f serving/third_party/istio-1.1-latest/istio.yaml
wait-pods --all-namespaces

kubectl apply -f serving/third_party/cert-manager-0.6.1/cert-manager-crds.yaml
wait-crd certificates.certmanager.k8s.io
kubectl apply -f serving/third_party/cert-manager-0.6.1/cert-manager.yaml --validate=false
wait-pods --all-namespaces

# Set a branch/tag
# REV=master for d in *; do git -C $d checkout $REV; done
ko apply -f serving/config/ && wait-pods --all-namespaces
ko apply -f eventing/config/ && wait-pods --all-namespaces
ko apply -f eventing/config/provisioners/in-memory-channel/ && wait-pods --all-namespaces
ko apply -f eventing-sources/config/ && wait-pods --all-namespaces

# Running tests
cd eventing
sh test/upload-test-images.sh latest
time go test -v -tags=e2e -count=1 -short -parallel=1 ./...
```

# Re-setting the cluster

This should shut down your cluster and remove all config changes made by kubeadm:

```
for h in node1 node2&& do
    kubectl drain $h --delete-local-data --force --ignore-daemonsets
    kubectl delete node $h
done
for h in $HOSTS; do ssh root@$h kubeadm reset --force; done
```

The following extra steps are for people with trust issues. I am not
paranoid, they really are out to get me. Don't come crying to me if
this turns your servers into paperweights.

```
for h in $HOSTS; do
    ssh root@$h "dnf erase -y 'kube*' flannel etcd docker-ce docker-ce-cli"
    ssh root@$h rm -rfv /etc/kubernetes /usr/libexec/kubernetes /etc/docker */var/lib/docker /usr/libexec/docker /etc/etcd /etc/sysconfig/flanneld /var/lib/cni
    ssh root@$h 'iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X'
done
for h in $HOSTS; do ssh root@$h reboot now; done
```

# Other resources

The following guides are similar to this one, that I found them later:
* https://unofficial-kubernetes.readthedocs.io/en/latest/getting-started-guides/kubeadm/
* https://developer.ibm.com/tutorials/developing-a-kubernetes-application-with-local-and-remote-clusters/
